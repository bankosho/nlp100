{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50. 文区切り"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing\n",
      "From Wikipedia, the free encyclopedia\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\n",
      "As such, NLP is related to the area of humani-computer interaction.\n",
      "Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\n",
      "History\n",
      "The history of NLP generally starts in the 1950s, although work can be found from earlier periods.\n",
      "In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.\n",
      "The authors claimed that within three or five years, machine translation would be a solved problem.\n",
      "However, real progress was much slower, and after the ALPAC report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def nlp_lines():\n",
    "    with open(\"nlp.txt\") as f:\n",
    "        pattern=re.compile(r'(^.*?[\\.|\\;|\\?|\\!])\\s([A-Z].*)')\n",
    "        for line in f:\n",
    "            line=line.rstrip()    \n",
    "            while len(line)>0:\n",
    "                match=pattern.match(line)\n",
    "                if match:\n",
    "                    yield match.group(1)\n",
    "                    line=match.group(2)\n",
    "                else:\n",
    "                    yield line\n",
    "                    line=''\n",
    "for i,line in enumerate(nlp_lines()):\n",
    "    print(line)\n",
    "    if i==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 51. 単語の切り出し"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\n",
      "language\n",
      "processing\n",
      "\n",
      "From\n",
      "Wikipedia\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "(NLP)\n",
      "is\n",
      "a\n",
      "field\n",
      "of\n",
      "computer\n",
      "science\n",
      "artificial\n"
     ]
    }
   ],
   "source": [
    "def nlp_words():\n",
    "    for line in nlp_lines():\n",
    "        for word in line.split():\n",
    "            yield word.rstrip(\".,;:?!\")\n",
    "        yield ''\n",
    "for i,word in enumerate(nlp_words()):\n",
    "    print(word)\n",
    "    if i==20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 52. ステミング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ． Pythonでは，Porterのステミングアルゴリズムの実装としてstemmingモジュールを利用するとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\tNatur\n",
      "language\tlanguag\n",
      "processing\tprocess\n",
      "\t\n",
      "From\tFrom\n",
      "Wikipedia\tWikipedia\n",
      "the\tthe\n",
      "free\tfree\n",
      "encyclopedia\tencyclopedia\n",
      "\t\n",
      "Natural\tNatur\n",
      "language\tlanguag\n",
      "processing\tprocess\n",
      "(NLP)\t(NLP)\n",
      "is\tis\n",
      "a\ta\n",
      "field\tfield\n",
      "of\tof\n",
      "computer\tcomput\n",
      "science\tscienc\n",
      "artificial\tartifici\n",
      "intelligence\tintellig\n",
      "and\tand\n",
      "linguistics\tlinguist\n",
      "concerned\tconcern\n",
      "with\twith\n",
      "the\tthe\n",
      "interactions\tinteract\n",
      "between\tbetween\n",
      "computers\tcomput\n",
      "and\tand\n"
     ]
    }
   ],
   "source": [
    "import snowballstemmer\n",
    "stemmer=snowballstemmer.stemmer('english')\n",
    "for i,word in enumerate(nlp_words()):\n",
    "    print('{}\\t{}'.format(word,stemmer.stemWord(word)))\n",
    "    if i==30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 53. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Core NLPを用い，入力テキストの解析結果をXML形式で得よ．また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\n",
      "language\n",
      "processing\n",
      "From\n",
      "Wikipedia\n",
      ",\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "-LRB-\n",
      "NLP\n",
      "-RRB-\n",
      "is\n",
      "a\n",
      "field\n",
      "of\n",
      "computer\n",
      "science\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree=ET.parse('nlp.txt.xml')\n",
    "root=tree.getroot()\n",
    "i=0\n",
    "for word in root.findall(\".//word\"):\n",
    "    print(word.text)\n",
    "    i+=1\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 54. 品詞タグ付け"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Core NLPの解析結果XMLを読み込み，単語，レンマ，品詞をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\tnatural\tJJ\n",
      "language\tlanguage\tNN\n",
      "processing\tprocessing\tNN\n",
      "From\tfrom\tIN\n",
      "Wikipedia\tWikipedia\tNNP\n",
      ",\t,\t,\n",
      "the\tthe\tDT\n",
      "free\tfree\tJJ\n",
      "encyclopedia\tencyclopedia\tNN\n",
      "Natural\tnatural\tJJ\n",
      "language\tlanguage\tNN\n",
      "processing\tprocessing\tNN\n",
      "-LRB-\t-lrb-\t-LRB-\n",
      "NLP\tnlp\tNN\n",
      "-RRB-\t-rrb-\t-RRB-\n",
      "is\tbe\tVBZ\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for token in root.findall(\".//token\"):\n",
    "    word=token.find('word').text\n",
    "    lemma=token.find('lemma').text\n",
    "    pos=token.find('POS').text\n",
    "    print(\"{}\\t{}\\t{}\".format(word,lemma,pos))\n",
    "    i+=1\n",
    "    if i>15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 55. 固有表現抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力文中の人名をすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan\n",
      "Turing\n",
      "Joseph\n",
      "Weizenbaum\n",
      "MARGIE\n",
      "Schank\n",
      "Wilensky\n",
      "Meehan\n",
      "Lehnert\n",
      "Carbonell\n",
      "Lehnert\n",
      "Racter\n",
      "Jabberwacky\n",
      "Moore\n"
     ]
    }
   ],
   "source": [
    "for token in root.findall(\".//token\"):\n",
    "    if token.find(\"NER\").text==\"PERSON\":\n",
    "        print(token.find(\"word\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 56. 共参照解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing From Wikipedia , the free encyclopedia Natural language processing -LRB- NLP -RRB- is the free encyclopedia Natural language processing -LRB- NLP -RRB-(a field of computer science) , artificial intelligence , and linguistics concerned with the interactions between computers and human -LRB- natural -RRB- languages . \n",
      "As such , NLP is related to the area of humani-computer interaction . \n",
      "Many challenges in NLP involve natural language understanding , that is , enabling computers(computers) to derive meaning from human or natural language input , and others involve natural language generation . \n",
      "History The history of NLP generally starts in the 1950s , although work can be found from earlier periods . \n",
      "In 1950 , Alan Turing published an article titled `` Computing Machinery and Intelligence '' which proposed what is now called the Alan Turing(Turing) test as a criterion of intelligence . \n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English . \n"
     ]
    }
   ],
   "source": [
    "rep_dict={}\n",
    "for coreference in root.findall(\"./document/coreference/coreference\"):\n",
    "    i=0\n",
    "    for mention in coreference.findall(\"mention\"):\n",
    "        if i==0:\n",
    "            rep_text=mention.findtext(\"text\")\n",
    "            i+=1\n",
    "        else:\n",
    "            sent_id=int(mention.findtext(\"sentence\"))\n",
    "            start=int(mention.findtext(\"start\"))\n",
    "            end=int(mention.findtext(\"end\"))\n",
    "            rep_dict[(sent_id,start)]=(end,rep_text)\n",
    "j=0\n",
    "for sentence in root.findall('./document/sentences/sentence'):\n",
    "    sent_id=int(sentence.get('id'))\n",
    "    b=0 \n",
    "\n",
    "    for token in sentence.iterfind('./tokens/token'):\n",
    "        token_id = int(token.get('id'))     \n",
    "\n",
    "        if (sent_id,token_id) in rep_dict:\n",
    "            (end, rep_text)=rep_dict[(sent_id,token_id)]\n",
    "            print(rep_text+'(',end='')\n",
    "            b=end-token_id \n",
    "        print(token.findtext('word'),end='')\n",
    "        if b>0:\n",
    "            b-=1\n",
    "            if b==0:\n",
    "                print(')',end='')\n",
    "        print(' ',end='')\n",
    "    print() \n",
    "    j+=1\n",
    "    if j>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 57. 係り受け解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphs.png'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "G=Digraph(format=\"png\")\n",
    "G.attr('node',shape='circle')\n",
    "dependencies=root.findall(\"./document/sentences/sentence[@id='1']/dependencies[@type='collapsed-dependencies']/dep\")\n",
    "for a in dependencies:\n",
    "    if a.get(\"type\")!=\"punct\":\n",
    "        G.node(a.find(\"./governor\").get(\"idx\"),a.findtext(\"governor\"))\n",
    "        G.node(a.find(\"./dependent\").get(\"idx\"),a.findtext(\"dependent\"))\n",
    "        G.edge(a.find(\"./governor\").get(\"idx\"),a.find(\"./dependent\").get(\"idx\"))\n",
    "G.render(\"graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 58. タプルの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding\tenabling\tcomputers\n",
      "others\tinvolve\tgeneration\n",
      "Turing\tpublished\tarticle\n",
      "experiment\tinvolved\ttranslation\n",
      "ELIZA\tprovided\tinteraction\n",
      "patient\texceeded\tbase\n",
      "ELIZA\tprovide\tresponse\n",
      "which\tstructured\tinformation\n",
      "underpinnings\tdiscouraged\tsort\n",
      "that\tunderlies\tapproach\n",
      "Some\tproduced\tsystems\n",
      "which\tmake\tdecisions\n",
      "systems\trely\twhich\n",
      "that\tcontains\terrors\n",
      "implementations\tinvolved\tcoding\n",
      "algorithms\ttake\tset\n",
      "Some\tproduced\tsystems\n",
      "which\tmake\tdecisions\n",
      "models\thave\tadvantage\n",
      "they\texpress\tcertainty\n",
      "Systems\thave\tadvantages\n",
      "Automatic\tmake\tuse\n",
      "that\tmake\tdecisions\n"
     ]
    }
   ],
   "source": [
    "k=1\n",
    "for sentence in root.findall(\"./document/sentences/sentence\"):\n",
    "    dict_1={}\n",
    "    dict_2={}\n",
    "    dict_3={}\n",
    "    for dep in sentence.findall(\"./dependencies[@type='collapsed-dependencies']/dep\"):\n",
    "        if dep.get(\"type\")==\"nsubj\" or dep.get(\"type\")==\"dobj\":\n",
    "            dongci=dep.find(\"governor\").text\n",
    "            idx=dep.find(\"governor\").get(\"idx\")\n",
    "            dict_1[idx]=dongci\n",
    "            if dep.get(\"type\")==\"nsubj\":\n",
    "                dict_2[idx] = dep.find('./dependent').text\n",
    "            else:\n",
    "                dict_3[idx] = dep.find('./dependent').text\n",
    "    for idx,dongci in dict_1.items():\n",
    "        nsubj=dict_2.get(idx)\n",
    "        dobj=dict_3.get(idx)\n",
    "        if nsubj is not None and dobj is not None:\n",
    "            print(\"{}\\t{}\\t{}\".format(nsubj,dongci,dobj))\n",
    "    k+=1\n",
    "    if k>50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 59. S式の解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Core NLPの句構造解析の結果（S式）を読み込み，文中のすべての名詞句（NP）を表示せよ．入れ子になっている名詞句もすべて表示すること．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "such\n",
      "NLP\n",
      "the area\n",
      "humani-computer interaction\n",
      "the area of humani-computer interaction\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern=re.compile(r\"^\\((.*?)\\s(.*)\\)\",re.DOTALL)\n",
    "def s_parse(str1,list_np):\n",
    "    word=[]\n",
    "    match=pattern.match(str1)\n",
    "    tag=match.group(1)\n",
    "    value=match.group(2)\n",
    "    depth=0\n",
    "    chunk=''\n",
    "    words=[]\n",
    "    for c in value:\n",
    "        if c==\"(\":\n",
    "            chunk+=c\n",
    "            depth+=1\n",
    "        elif c==')':\n",
    "            chunk+=c\n",
    "            depth-=1\n",
    "            if depth==0:\n",
    "                words.append(s_parse(chunk, list_np))\n",
    "                chunk=''\n",
    "        else:\n",
    "            if not(depth==0 and c==' '):\n",
    "                chunk+=c\n",
    "    if chunk!='':\n",
    "        words.append(chunk)\n",
    "    result=' '.join(words)\n",
    "    if tag=='NP':\n",
    "        list_np.append(result)\n",
    "    return result\n",
    "        \n",
    "    \n",
    "for s in root.findall(\"./document/sentences/sentence[@id='2']/parse\"):\n",
    "    result=[]\n",
    "    s_parse(s.text,result)\n",
    "    for a in result:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
